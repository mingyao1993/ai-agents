{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d1beab-f80c-4877-b335-4fa14391e52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Agent Framework: Author and deploy a multi-agent system with Genie and Serving Endpoints\n",
    "\n",
    "This notebook demonstrates how to build a multi-agent system using Mosaic AI Agent Framework and [LangGraph](https://blog.langchain.dev/langgraph-multi-agent-workflows/), where [Genie](https://www.databricks.com/product/ai-bi/genie) is one of the agents.\n",
    "In this notebook, you:\n",
    "1. Author a multi-agent system using LangGraph.\n",
    "1. Wrap the LangGraph agent with MLflow `ResponsesAgent` to ensure compatibility with Databricks features.\n",
    "1. Manually test the multi-agent system's output.\n",
    "1. Log and deploy the multi-agent system.\n",
    "\n",
    "This example is based on [LangGraph documentation - Multi-agent supervisor example](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/multi_agent/agent_supervisor.md)\n",
    "\n",
    "## Why use a Genie agent?\n",
    "\n",
    "Multi-agent systems consist of multiple AI agents working together, each with specialized capabilities. As one of those agents, Genie allows users to interact with their structured data using natural language. Unlike SQL functions which can only run pre-defined queries, Genie has the flexibility to create novel queries to answer user questions.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook.\n",
    "- Create a Genie Space, see Databricks documentation ([AWS](https://docs.databricks.com/aws/genie/set-up) | [Azure](https://learn.microsoft.com/azure/databricks/genie/set-up))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0979df09-2167-48b8-84be-4c51e0d513bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq langgraph-supervisor==0.0.30 mlflow[databricks] databricks-langchain databricks-agents uv \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc47ced-a71e-487b-99cb-448cdbc9f519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define the multi-agent system\n",
    "\n",
    "Create a multi-agent system in LangGraph using a supervisor agent node with one or more of the following subagents:\n",
    "- **GenieAgent**: A LangChain runnable that allows you to easily interact with your Genie Space to query structured data.\n",
    "- **Custom serving agent**: An agent that is already hosted as an existing endpoint on Databricks.\n",
    "- **In-code tool-calling agent**: An agent that calls Unity Catalog function tools, defined within this notebook. This example uses `system.ai.python_exec`, but for examples of other tools you can add to your agents, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-tool) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/agent-tool)).\n",
    "\n",
    "The supervisor agent is responsible for creating and routing tool calls to each of your subagents, passing only the context necessary. You can modify this behavior and pass along the entire message history if desired. See the [LangGraph docs](https://langchain-ai.github.io/langgraph/reference/supervisor/) for more information.\n",
    "\n",
    "### Write agent code to file\n",
    "\n",
    "Define the agent code in a single cell below. This lets you write the agent code to a local Python file, using the `%%writefile` magic command, for subsequent logging and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320493ee-8d7b-4b1d-b8c8-e31fdd2a6f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import json\n",
    "from typing import Generator, Literal\n",
    "from uuid import uuid4\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "########################################\n",
    "# Create your LangGraph Supervisor Agent\n",
    "########################################\n",
    "\n",
    "GENIE = \"genie\"\n",
    "\n",
    "\n",
    "class ServedSubAgent(BaseModel):\n",
    "    endpoint_name: str\n",
    "    name: str\n",
    "    task: Literal[\"agent/v1/responses\", \"agent/v1/chat\", \"agent/v2/chat\"]\n",
    "    description: str\n",
    "\n",
    "\n",
    "class Genie(BaseModel):\n",
    "    space_id: str\n",
    "    name: str\n",
    "    task: str = GENIE\n",
    "    description: str\n",
    "\n",
    "\n",
    "class InCodeSubAgent(BaseModel):\n",
    "    tools: list[str]\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "TOOLS = []\n",
    "\n",
    "\n",
    "def stringify_content(state):\n",
    "    msgs = state[\"messages\"]\n",
    "    if isinstance(msgs[-1].content, list):\n",
    "        msgs[-1].content = json.dumps(msgs[-1].content[-1], indent=4)\n",
    "    return {\"messages\": msgs}\n",
    "\n",
    "def create_langgraph_supervisor(\n",
    "    llm: Runnable,\n",
    "    externally_served_agents: list[ServedSubAgent] = [],\n",
    "    in_code_agents: list[InCodeSubAgent] = [],\n",
    "):\n",
    "    agents = []\n",
    "    agent_descriptions = \"\"\n",
    "\n",
    "    # Process inline code agents\n",
    "    for agent in in_code_agents:\n",
    "        agent_descriptions += f\"- {agent.name}: {agent.description}\\n\"\n",
    "        uc_toolkit = UCFunctionToolkit(function_names=agent.tools)\n",
    "        TOOLS.extend(uc_toolkit.tools)\n",
    "        agents.append(create_agent(llm, tools=uc_toolkit.tools, name=agent.name))\n",
    "\n",
    "    # Process served endpoints and Genie Spaces\n",
    "    for agent in externally_served_agents:\n",
    "        agent_descriptions += f\"- {agent.name}: {agent.description}\\n\"\n",
    "        if isinstance(agent, Genie):\n",
    "            # to better control the messages sent to the genie agent, you can use the `message_processor` param: https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.GenieAgent\n",
    "            genie_agent = GenieAgent(\n",
    "                genie_space_id=agent.space_id,\n",
    "                genie_agent_name=agent.name,\n",
    "                description=agent.description,\n",
    "            )\n",
    "            genie_agent.name = agent.name\n",
    "            agents.append(genie_agent)\n",
    "        else:\n",
    "            model = ChatDatabricks(\n",
    "                endpoint=agent.endpoint_name, use_responses_api=\"responses\" in agent.task\n",
    "            )\n",
    "            # Disable streaming for subagents for ease of parsing\n",
    "            model._stream = lambda x: model._stream(**x, stream=False)\n",
    "            agents.append(\n",
    "                create_agent(\n",
    "                    model,\n",
    "                    tools=[],\n",
    "                    name=agent.name,\n",
    "                    post_model_hook=stringify_content,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # TODO: The supervisor prompt includes agent names/descriptions as well as general\n",
    "    # instructions. You can modify this to improve quality or provide custom instructions.\n",
    "    prompt = f\"\"\"\n",
    "    You are a supervisor in a multi-agent system.\n",
    "\n",
    "    1. Understand the user's last request\n",
    "    2. Read through the entire chat history.\n",
    "    3. If the answer to the user's last request is present in chat history, answer using information in the history.\n",
    "    4. If the answer is not in the history, from the below list of agents, determine which agent is best suited to answer the question.\n",
    "    5. Provide a summarized response to the user's last query, even if it's been answered before.\n",
    "\n",
    "    {agent_descriptions}\"\"\"\n",
    "\n",
    "    return create_supervisor(\n",
    "        agents=agents,\n",
    "        model=llm,\n",
    "        prompt=prompt,\n",
    "        add_handoff_messages=False,\n",
    "        output_mode=\"full_history\",\n",
    "    ).compile()\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Wrap LangGraph Supervisor as a ResponsesAgent\n",
    "##########################################\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        print(\"This is request.input:\",request.input)\n",
    "        print(\"This is cc_msgs:\",cc_msgs)\n",
    "        first_message = True\n",
    "        seen_ids = set()\n",
    "\n",
    "        # can adjust `recursion_limit` to limit looping: https://docs.langchain.com/oss/python/langgraph/GRAPH_RECURSION_LIMIT#troubleshooting\n",
    "        for _, events in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\"]):\n",
    "            new_msgs = [\n",
    "                msg\n",
    "                for v in events.values()\n",
    "                for msg in v.get(\"messages\", [])\n",
    "                if msg.id not in seen_ids\n",
    "            ]\n",
    "            print(\"this is new msgs:\", new_msgs)\n",
    "            if first_message:\n",
    "                seen_ids.update(msg.id for msg in new_msgs[: len(cc_msgs)])\n",
    "                new_msgs = new_msgs[len(cc_msgs) :]\n",
    "                first_message = False\n",
    "            else:\n",
    "                seen_ids.update(msg.id for msg in new_msgs)\n",
    "                node_name = tuple(events.keys())[0]  # assumes one name per node\n",
    "                yield ResponsesAgentStreamEvent(\n",
    "                    type=\"response.output_item.done\",\n",
    "                    item=self.create_text_output_item(\n",
    "                        text=f\"<name>{node_name}</name>\", id=str(uuid4())\n",
    "                    ),\n",
    "                )\n",
    "            if len(new_msgs) > 0:\n",
    "                def format_msg_content(msg):\n",
    "                    if isinstance(msg.content, str):\n",
    "                        try:\n",
    "                            msg.content = json.loads(msg.content)\n",
    "                            for item in msg.content:\n",
    "                                if item.get(\"type\") == \"text\":\n",
    "                                    msg.content = item[\"text\"]\n",
    "                                    break\n",
    "                        except (json.JSONDecodeError, TypeError):\n",
    "                            pass\n",
    "                    return msg\n",
    "                new_msgs = [format_msg_content(msg) for msg in new_msgs]\n",
    "                yield from output_to_responses_items_stream(new_msgs)\n",
    "\n",
    "# this is new msgs: [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='976716e0-7779-4c8b-836d-3f8d478538a8'), AIMessage(content='[{\"type\": \"reasoning\", \"summary\": [{\"type\": \"summary_text\", \"text\": \"We have a conversation: user says \\\\\"Hi\\\\\". No earlier messages. The system says you are a supervisor in a multi-agent system, and you must produce a summarized response to the user\\'s last query; the last query is \\\\\"Hi\\\\\". There\\'s no answer in history. We must decide which agent is best suited to answer \\\\\"Hi\\\\\". Likely a greeting agent. Let\\'s refer to list: but no list given. So we need to output a greeting. The user is greeting. So respond appropriately. So best agent likely is the greeting chat. So we output greeting.\"}]}, {\"type\": \"text\", \"text\": \"Hey there! \\\\ud83d\\\\udc4b How can I help you today?\"}]', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 201, 'completion_tokens': 135, 'total_tokens': 336}, 'prompt_tokens': 201, 'completion_tokens': 135, 'total_tokens': 336, 'model': 'gpt-oss-20b-080525', 'model_name': 'gpt-oss-20b-080525', 'finish_reason': 'stop'}, name='supervisor', id='lc_run--019b5828-ce0a-7de1-b0a1-313ae64c948f-0')]\n",
    "\n",
    "#######################################################\n",
    "# Configure the Foundation Model and Serving Sub-Agents\n",
    "#######################################################\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-gpt-oss-20b\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Add the necessary information about each of your subagents. Subagents could be agents deployed to Model Serving endpoints or Genie Space subagents.\n",
    "# Your agent descriptions are crucial for improving quality. Include as much detail as possible.\n",
    "EXTERNALLY_SERVED_AGENTS = [\n",
    "    # Genie(\n",
    "    #     space_id=\"<your_genie_space_id>\",\n",
    "    #     name=\"<your-genie-name>\",\n",
    "    #     description=\"This agent can answer questions...\",\n",
    "    # ),\n",
    "    # ServedSubAgent(\n",
    "    #     endpoint_name=\"cities-agent\",\n",
    "    #     name=\"city-agent\", # choose a semantically relevant name for your agent\n",
    "    #     task=\"agent/v1/responses\",\n",
    "    #     description=\"This agent can answer questions about the best cities to visit in the world.\",\n",
    "    # ),\n",
    "]\n",
    "\n",
    "############################################################\n",
    "# Create additional agents in code\n",
    "############################################################\n",
    "\n",
    "# TODO: Fill the following with UC function-calling agents. The tools parameter is a list of UC function names that you want your agent to call.\n",
    "IN_CODE_AGENTS = [\n",
    "    # InCodeSubAgent(\n",
    "    #     tools=[\"system.ai.*\"],\n",
    "    #     name=\"code execution agent\",\n",
    "    #     description=\"The code execution agent specializes in solving programming challenges, generating code snippets, debugging issues, and explaining complex coding concepts.\",\n",
    "    # )\n",
    "]\n",
    "\n",
    "#################################################\n",
    "# Create supervisor and set up MLflow for tracing\n",
    "#################################################\n",
    "\n",
    "supervisor = create_langgraph_supervisor(llm, EXTERNALLY_SERVED_AGENTS, IN_CODE_AGENTS)\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphResponsesAgent(supervisor)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f12478-4528-4c54-9089-9f24353929e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since this notebook called `mlflow.langchain.autolog()` you can view the trace for each step the agent takes.\n",
    "\n",
    "Even if you didn't add any subagents in the agent definition above, the supervisor agent can still answer questions. It just won't have any subagents to switch to.\n",
    "\n",
    "**Important:** LangGraph internally uses exceptions (something like `Command` or `ParentCommand`) to switch between nodes. These particular exceptions may appear in your MLflow traces as Events, but this behavior is expected and should not be a cause for concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80b998f-bf9e-4999-b4a7-6f3ebb309f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3342568-80f1-4877-a234-7f5635a61a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "# TODO: Replace this placeholder `input_example` with a domain-specific prompt for your agent.\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hi\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "AGENT.predict(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d877df-f40e-4f3d-98f9-f9638bea49cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for event in AGENT.predict_stream(input_example):\n",
    "  print(event.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005e30fb-0ea9-453f-8296-0fa5b5db00d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "### Enable automatic authentication for Databricks resources\n",
    "For the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent. With automatic authentication passthrough, Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n",
    "\n",
    "To enable automatic authentication, specify the dependent Databricks resources when calling `mlflow.pyfunc.log_model().`\n",
    "  - **TODO**: If your Unity Catalog tool queries a [vector search index](docs link) or leverages [external functions](docs link), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See docs ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-authentication#supported-resources-for-automatic-authentication-passthrough) | [Azure](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-authentication#supported-resources-for-automatic-authentication-passthrough)).\n",
    "\n",
    "  - **TODO**: Add the SQL Warehouse or tables powering your Genie space to enable passthrough authentication. ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-authentication#supported-resources-for-automatic-authentication-passthrough) | [Azure](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-authentication#supported-resources-for-automatic-authentication-passthrough)). If your genie space uses \"embedded credentials\" then you do not have to add this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ba7e01-5ea6-46e1-858e-cf0f84540b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import mlflow\n",
    "from agent import EXTERNALLY_SERVED_AGENTS, LLM_ENDPOINT_NAME, TOOLS, Genie\n",
    "from databricks_langchain import UnityCatalogTool, VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksFunction,\n",
    "    DatabricksGenieSpace,\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksSQLWarehouse,\n",
    "    DatabricksTable\n",
    ")\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# TODO: Manually include underlying resources if needed. See the TODO in the markdown above for more information.\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "# TODO: Add SQL Warehouses and delta tables powering the Genie Space\n",
    "# resources.append(DatabricksSQLWarehouse(warehouse_id=\"<your_warehouse_id>\"))\n",
    "# resources.append(DatabricksTable(table_name=\"<your_catalog>.<schema>.<table_name>\"))\n",
    "\n",
    "# Add tools from Unity Catalog\n",
    "for tool in TOOLS:\n",
    "    if isinstance(tool, VectorSearchRetrieverTool):\n",
    "        resources.extend(tool.resources)\n",
    "    elif isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "# Add serving endpoints and Genie Spaces\n",
    "for agent in EXTERNALLY_SERVED_AGENTS:\n",
    "    if isinstance(agent, Genie):\n",
    "        resources.append(DatabricksGenieSpace(genie_space_id=agent.space_id))\n",
    "    else:\n",
    "        resources.append(DatabricksServingEndpoint(endpoint_name=agent.endpoint_name))\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"langgraph-supervisor=={get_distribution('langgraph-supervisor').version}\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdafe2f-21c7-49ee-9452-10e421edbd74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-deployment agent validation\n",
    "Before registering and deploying the agent, perform pre-deployment checks using the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/model-serving-debug.html#validate-inputs) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/model-serving-debug#before-model-deployment-validation-checks))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee9570d-d518-4ecc-aded-0f1dd3bc3717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data=input_example,\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6dd216-5cd3-44cc-a3bc-62c0bcb7ff9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a70719d-5fac-4c12-8694-6002073fcb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"workspace\"\n",
    "schema = \"default\"\n",
    "model_name = \"langgraph-multiagent-genie-example\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3f5e36-7750-4b0f-b354-9fba7f7aa549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98bc4e5e-774b-4ba3-89d1-e02b9599e43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    tags={\"endpointSource\": \"docs\"},\n",
    "    deploy_feedback_model=False,\n",
    "    scale_to_zero=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2253abb4-6922-47a2-bf4c-f69e3590f481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/deploy-agent.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/deploy-agent))."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "langgraph-multiagent-genie",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
